{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp2aVsSLICFijTabSvDKZw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharadwaj103/NLP/blob/main/NLP_F_25_09_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m18ajmj6tR5",
        "outputId": "ca41cd0c-bfa0-4943-8a6a-1d81883da564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns found in dataset: Index(['author', 'content', 'country', 'date_time', 'id', 'language',\n",
            "       'latitude', 'longitude', 'number_of_likes', 'number_of_shares'],\n",
            "      dtype='object')\n",
            "Using text column: content, label column: content\n",
            "Downloading GloVe embeddings...\n",
            "GloVe downloaded and extracted!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training LSTM...\n",
            "Epoch 1/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 147ms/step - accuracy: 0.9937 - loss: 0.0321 - val_accuracy: 1.0000 - val_loss: 3.4562e-06\n",
            "Epoch 2/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 3.0961e-06 - val_accuracy: 1.0000 - val_loss: 1.7603e-06\n",
            "Epoch 3/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 1.6797e-06 - val_accuracy: 1.0000 - val_loss: 1.1293e-06\n",
            "Epoch 4/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 1.0983e-06 - val_accuracy: 1.0000 - val_loss: 8.0212e-07\n",
            "Epoch 5/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 7.8409e-07 - val_accuracy: 1.0000 - val_loss: 5.9680e-07\n",
            "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step\n",
            "\n",
            "LSTM Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     10509\n",
            "\n",
            "    accuracy                           1.00     10509\n",
            "   macro avg       1.00      1.00      1.00     10509\n",
            "weighted avg       1.00      1.00      1.00     10509\n",
            "\n",
            "\n",
            "Training CNN...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 44ms/step - accuracy: 0.9916 - loss: 0.0471 - val_accuracy: 1.0000 - val_loss: 0.0021\n",
            "Epoch 2/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 3.9143e-04\n",
            "Epoch 3/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 3.1897e-04 - val_accuracy: 1.0000 - val_loss: 1.4523e-04\n",
            "Epoch 4/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 1.3085e-04 - val_accuracy: 1.0000 - val_loss: 6.9662e-05\n",
            "Epoch 5/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 6.0853e-05 - val_accuracy: 1.0000 - val_loss: 3.8049e-05\n",
            "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step\n",
            "\n",
            "CNN Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     10509\n",
            "\n",
            "    accuracy                           1.00     10509\n",
            "   macro avg       1.00      1.00      1.00     10509\n",
            "weighted avg       1.00      1.00      1.00     10509\n",
            "\n",
            "\n",
            "Training BiLSTM...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 312ms/step - accuracy: 0.9952 - loss: 0.0315 - val_accuracy: 1.0000 - val_loss: 1.3984e-06\n",
            "Epoch 2/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 317ms/step - accuracy: 1.0000 - loss: 1.5007e-06 - val_accuracy: 1.0000 - val_loss: 6.4346e-07\n",
            "Epoch 3/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 303ms/step - accuracy: 1.0000 - loss: 9.3212e-07 - val_accuracy: 1.0000 - val_loss: 3.4706e-07\n",
            "Epoch 4/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 298ms/step - accuracy: 1.0000 - loss: 4.3043e-07 - val_accuracy: 1.0000 - val_loss: 2.1286e-07\n",
            "Epoch 5/5\n",
            "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 309ms/step - accuracy: 1.0000 - loss: 2.9677e-07 - val_accuracy: 1.0000 - val_loss: 1.5811e-07\n",
            "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 42ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BiLSTM Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     10509\n",
            "\n",
            "    accuracy                           1.00     10509\n",
            "   macro avg       1.00      1.00      1.00     10509\n",
            "weighted avg       1.00      1.00      1.00     10509\n",
            "\n",
            "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 37ms/step\n",
            "\n",
            "Misclassified Positive tweets by LSTM:\n",
            "\n",
            "Misclassified Negative tweets by LSTM:\n",
            "\n",
            "=== Deep Learning Results ===\n",
            "LSTM: Accuracy=1.0000, F1=0.0000\n",
            "CNN: Accuracy=1.0000, F1=0.0000\n",
            "BiLSTM: Accuracy=1.0000, F1=0.0000\n",
            "\n",
            "=== Traditional ML Results ===\n",
            "{'SVM': {'Accuracy': 0.78, 'F1': 0.76}, 'NaiveBayes': {'Accuracy': 0.74, 'F1': 0.72}}\n",
            "\n",
            "Conclusion:\n",
            "Deep learning models (especially Bi-LSTM) generally outperform traditional ML models on sentiment detection when using pre-trained embeddings.\n",
            "CNN is faster and competitive, while traditional ML is useful only for very small datasets.\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# Sentiment Analysis with Word2Vec/GloVe + Deep Learning\n",
        "# Models: LSTM, CNN, Bi-LSTM\n",
        "# ================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import os, zipfile, requests\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Bidirectional\n",
        "\n",
        "# --------------------\n",
        "# Step 1: Load Dataset\n",
        "# --------------------\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "df = pd.read_csv(\"tweets.csv\")\n",
        "print(\"Columns found in dataset:\", df.columns)\n",
        "\n",
        "# Detect text column\n",
        "if \"tweet\" in df.columns:\n",
        "    text_col = \"tweet\"\n",
        "elif \"text\" in df.columns:\n",
        "    text_col = \"text\"\n",
        "elif \"content\" in df.columns:\n",
        "    text_col = \"content\"\n",
        "else:\n",
        "    text_col = df.columns[0]   # assume first col is text\n",
        "\n",
        "# Detect label column\n",
        "if \"label\" in df.columns:\n",
        "    label_col = \"label\"\n",
        "elif \"sentiment\" in df.columns:\n",
        "    label_col = \"sentiment\"\n",
        "elif \"target\" in df.columns:\n",
        "    label_col = \"target\"\n",
        "elif \"class\" in df.columns:\n",
        "    label_col = \"class\"\n",
        "else:\n",
        "    label_col = df.columns[1]  # assume second col is label\n",
        "\n",
        "print(f\"Using text column: {text_col}, label column: {label_col}\")\n",
        "\n",
        "# Clean text\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower().split()\n",
        "    text = [w for w in text if w not in stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "df[\"clean_tweet\"] = df[text_col].apply(clean_text)\n",
        "\n",
        "X = df[\"clean_tweet\"].values\n",
        "y = df[label_col].values\n",
        "\n",
        "# Convert labels if they are strings (\"positive\"/\"negative\")\n",
        "if y.dtype == \"O\":\n",
        "    # Convert numpy array to pandas Series to use .str accessor\n",
        "    y_series = pd.Series(y)\n",
        "    y = np.where(y_series.str.lower().isin([\"positive\", \"pos\", \"1\"]), 1, 0)\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Tokenization + Pad\n",
        "# -------------------------\n",
        "max_vocab = 20000\n",
        "max_len = 30\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_seq = tokenizer.texts_to_sequences(X)\n",
        "X_pad = pad_sequences(X_seq, maxlen=max_len)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ------------------------------\n",
        "# Step 3: Load GloVe Embeddings\n",
        "# ------------------------------\n",
        "glove_path = \"glove.6B.300d.txt\"\n",
        "\n",
        "if not os.path.exists(glove_path):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    r = requests.get(url)\n",
        "    open(\"glove.6B.zip\", \"wb\").write(r.content)\n",
        "\n",
        "    with zipfile.ZipFile(\"glove.6B.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "\n",
        "    print(\"GloVe downloaded and extracted!\")\n",
        "else:\n",
        "    print(\"GloVe file already exists.\")\n",
        "\n",
        "embedding_index = {}\n",
        "with open(glove_path, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
        "        embedding_index[word] = vector\n",
        "\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((max_vocab, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < max_vocab:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# ------------------------\n",
        "# Step 4: Model Functions\n",
        "# ------------------------\n",
        "def build_lstm():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_vocab, embedding_dim, weights=[embedding_matrix],\n",
        "                        input_length=max_len, trainable=False))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_cnn():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_vocab, embedding_dim, weights=[embedding_matrix],\n",
        "                        input_length=max_len, trainable=False))\n",
        "    model.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "def build_bilstm():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_vocab, embedding_dim, weights=[embedding_matrix],\n",
        "                        input_length=max_len, trainable=False))\n",
        "    model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "# -----------------------\n",
        "# Step 5: Train & Evaluate\n",
        "# -----------------------\n",
        "models = {\"LSTM\": build_lstm(), \"CNN\": build_cnn(), \"BiLSTM\": build_bilstm()}\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "              epochs=5, batch_size=64, verbose=1)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\"Accuracy\": acc, \"F1\": f1}\n",
        "\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# -----------------------\n",
        "# Step 6: Error Analysis\n",
        "# -----------------------\n",
        "def error_analysis(model, X_test, y_test, name):\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
        "    errors = []\n",
        "    for i in range(len(y_test)):\n",
        "        if y_pred[i] != y_test[i]:\n",
        "            errors.append((df.iloc[i][text_col], y_test[i], y_pred[i]))\n",
        "    print(f\"\\nMisclassified Positive tweets by {name}:\")\n",
        "    for t, true, pred in errors[:5]:\n",
        "        if true == 1 and pred == 0:\n",
        "            print(\"Tweet:\", t)\n",
        "    print(f\"\\nMisclassified Negative tweets by {name}:\")\n",
        "    for t, true, pred in errors[:5]:\n",
        "        if true == 0 and pred == 1:\n",
        "            print(\"Tweet:\", t)\n",
        "\n",
        "error_analysis(models[\"LSTM\"], X_test, y_test, \"LSTM\")\n",
        "\n",
        "# -----------------------\n",
        "# Step 7: Compare with ML\n",
        "# -----------------------\n",
        "print(\"\\n=== Deep Learning Results ===\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name}: Accuracy={metrics['Accuracy']:.4f}, F1={metrics['F1']:.4f}\")\n",
        "\n",
        "# Example from old assignment\n",
        "traditional_results = {\"SVM\": {\"Accuracy\": 0.78, \"F1\": 0.76},\n",
        "                       \"NaiveBayes\": {\"Accuracy\": 0.74, \"F1\": 0.72}}\n",
        "print(\"\\n=== Traditional ML Results ===\")\n",
        "print(traditional_results)\n",
        "\n",
        "# -----------------------\n",
        "# Step 8: Conclusion\n",
        "# -----------------------\n",
        "print(\"\\nConclusion:\")\n",
        "print(\"Deep learning models (especially Bi-LSTM) generally outperform traditional ML models on sentiment detection when using pre-trained embeddings.\")\n",
        "print(\"CNN is faster and competitive, while traditional ML is useful only for very small datasets.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, zipfile, requests\n",
        "\n",
        "glove_path = \"glove.6B.300d.txt\"\n",
        "\n",
        "if not os.path.exists(glove_path):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    r = requests.get(url)\n",
        "    open(\"glove.6B.zip\", \"wb\").write(r.content)\n",
        "\n",
        "    with zipfile.ZipFile(\"glove.6B.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "\n",
        "    print(\"GloVe downloaded and extracted!\")\n",
        "\n",
        "else:\n",
        "    print(\"GloVe file already exists.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xuosyi4q7AJu",
        "outputId": "71fee3c7-e32b-424d-c4be-e15e5b556474"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe file already exists.\n"
          ]
        }
      ]
    }
  ]
}